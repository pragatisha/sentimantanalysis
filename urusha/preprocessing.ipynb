{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import nltk # python nlp libraries\n",
    "import pandas as pd  # data analysis and manipulation tool\n",
    "import re   # for regular expression operation\n",
    "import string # transform and remove punctuations\n",
    "import os   # operation system dependent functionality\n",
    "import csv  # parse csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Negative    17031\n",
      "Neutral      8332\n",
      "Positive    19592\n",
      "Name: OriginalTweet, dtype: int64\n",
      "OriginalTweet    24000\n",
      "Sentiment        24000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"Coronavirus-tweets.csv\", usecols=[\"Sentiment\", \"OriginalTweet\"]) # get data set\n",
    "# print (dataframe) # check if retrieved\n",
    "colsentiment = dataframe.groupby(\"Sentiment\")\n",
    "print (colsentiment[\"OriginalTweet\"].count())\n",
    "\n",
    "\n",
    "dfNegative = dataframe.loc[dataframe[\"Sentiment\"] == \"Negative\"].head(8000)\n",
    "dfNeutral = dataframe.loc[dataframe[\"Sentiment\"] == \"Neutral\"].tail(8000)\n",
    "dfPositive = dataframe.loc[dataframe[\"Sentiment\"] == \"Positive\"].sample(8000)\n",
    "\n",
    "df = [ dfNegative, dfNeutral, dfPositive]\n",
    "dataframe = pd.concat(df)\n",
    "\n",
    "print (dataframe.count())\n",
    "\n",
    "# print (dfExtremelyNegative)  # testing purpose only\n",
    "# print (dfExtremelyPositive.count())  # testing purpose only\n",
    "# print (dfNegative.count())  # testing purpose only\n",
    "# print (dfPositive.count())  # testing purpose only\n",
    "# print (dfNeutral.count())  # testing purpose only\n",
    "\n",
    "rawData = [list(x) for x in dataframe.values]  # convert to list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all raw data\n",
    "def FilterData(tweet):\n",
    "    # convert all to lowercase\n",
    "    lowcaseTweet = tweet.lower()\n",
    "     #  Removing all numbers\n",
    "    removednumbersTweet = re.sub(r'\\d+', '', lowcaseTweet)\n",
    "    # Remove punctuations\n",
    "    removepunctuationsTweet = removednumbersTweet.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    # Removing white spaces\n",
    "    removewhitespacesTweet = removepunctuationsTweet.strip()\n",
    "    # Removing all characters instead a string\n",
    "    #s = '@#24A-09=wes()&8973o**_##me'  # contains letters 'Awesome' for testing purposes \n",
    "   \n",
    "    # removing all urls\n",
    "    filteredData = re.sub(r'http\\S+', ' ', removewhitespacesTweet)\n",
    "    filteredData = re.sub(r'[^A-Za-z]', ' ', filteredData)\n",
    "\n",
    "#     print (filteredData) # testing purposes\n",
    "    return (filteredData)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise filtered data\n",
    "def tokenizeStr(data_str):\n",
    "    lst_tokens = []\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(data_str)\n",
    "    for each in tokens:\n",
    "        lst_tokens.append(each)\n",
    "    return lst_tokens\n",
    "\n",
    "def RemoveStopWords(lst_data_str):\n",
    "    lst_stop_word_removed = [i for i in lst_data_str if not i in set_stop_words]\n",
    "    return lst_stop_word_removed\n",
    "\n",
    "def StemWords(lst_data_str):\n",
    "#     stemmer = nltk.stem.PorterStemmer()\n",
    "#     stemtokens = (stemmer.stem(token) for token in lst_data_str)\n",
    "#     print (stemtokens)\n",
    "    lst_stemtokens = []\n",
    "#     print(lst_data_str)\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    stemtokens = (lemma.lemmatize(token) for token in lst_data_str)\n",
    "    \n",
    "    for each in stemtokens:\n",
    "        try:\n",
    "#             print (each)\n",
    "            lst_stemtokens.append(each.encode('utf-8'))\n",
    "        except:\n",
    "            print(\"exception\")\n",
    "       \n",
    "    return lst_stemtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise all review data\n",
    "def TextPreprocess (data_str):\n",
    "    lst_tokens = tokenizeStr(data_str)\n",
    "    lst_removed_stpwrd = RemoveStopWords(lst_tokens)\n",
    "    lst_stem_words = StemWords(lst_removed_stpwrd)\n",
    "    processedData = \" \".join(lst_stem_words)\n",
    "    return (processedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_stop_words = []\n",
    "def setStopWords():\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    for each in stop_words:\n",
    "        #     print (each)\n",
    "        set_stop_words.append(each.encode('utf-8')) # encode because data read u'all', u'why' representing string data\n",
    "\n",
    "setStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all data after cleaning and normalising.\n",
    "processedDataList = []\n",
    "processedDataList.append([\"OriginalTweets\", \"Sentiment\"])\n",
    "def NormaliseData (tweet):\n",
    "    tweetStatement = tweet[0]\n",
    "    sentiment = tweet[1]\n",
    "#     print (sentiment) # for testing purpose\n",
    "    normalisedData = FilterData (tweetStatement)\n",
    "    processedData = TextPreprocess (normalisedData)\n",
    "    processedDataCategoryList = [processedData, sentiment ]\n",
    "    processedDataList.append(processedDataCategoryList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachData in rawData:\n",
    "#     print (eachData)\n",
    "    NormaliseData (eachData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write review data to csv file\n",
    "writer = csv.writer(open('processed-data.csv','wb'))\n",
    "for each in processedDataList:\n",
    "    writer.writerow(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
